\documentclass[USenglish]{article}	
% for 2-column layout use \documentclass[USenglish,twocolumn]{article}

\usepackage[utf8]{inputenc}				%(only for the pdftex engine)
%\RequirePackage[no-math]{fontspec}[2017/03/31]%(only for the luatex or the xetex engine)
\usepackage[big,online]{dgruyter}	%values: small,big | online,print,work
\usepackage{lmodern} 
\usepackage{microtype}
\usepackage[numbers,square,sort&compress]{natbib}
\usepackage{algorithm}
\usepackage[utf8]{inputenc} % allow utf-8 input
% \usepackage[english, russian]{babel}	% локализация и переносы
% \usepackage[russian]{babel}	% локализация и переносы
%\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{lipsum}		% Can be removed after putting your text content
\usepackage{graphicx}
\usepackage{natbib}
\usepackage{doi}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{appendix}

\usepackage{caption}
\usepackage{subcaption}

% \documentclass{article}

\graphicspath{{pictures/}, {images/}, {}}
\DeclareGraphicsExtensions{.pdf,.png,.jpg}

% Цвета 
\usepackage[dvipsnames]{xcolor}
\usepackage{color}   


% New theorem-like environments will be introduced by using the commands \theoremstyle and \newtheorem.
% Please note that the environments proof and definition are already defined within dgryuter.sty.
\theoremstyle{dgthm}
\newtheorem{theorem}{Theorem}
\newtheorem{assumption}{Assumption}
\newtheorem{corollary}{Corollary}
\newtheorem{proposition}{Proposition}
\newtheorem{lemma}{Lemma}
\newtheorem{assertion}{Assertion}
\newtheorem{result}{Result}
\newtheorem{conclusion}{Conclusion}

\newtheorem{definition}{Definition}
\newtheorem{example}{Example}
\newtheorem{remark}{Remark}



\begin{document}

	
%%%--------------------------------------------%%%
	\articletype{Research Article}
	\received{Month	DD, YYYY}
	\revised{Month	DD, YYYY}
  \accepted{Month	DD, YYYY}
  \journalname{De~Gruyter~Journal}
  \journalyear{YYYY}
  \journalvolume{XX}
  \journalissue{X}
  \startpage{1}
  \aop
  \DOI{10.1515/sample-YYYY-XXXX}
%%%--------------------------------------------%%%
\title{Surrogate assisted diversity estimation in NES}
\runningtitle{Surrogate Function in Neural Networks}
%\subtitle{Insert subtitle if needed}

\author[1]{Alexandr Udeneev}
\author[2]{Petr Babkin}
\author[3]{Oleg Bahteev}


% \affil[1]{\protect\raggedright 
% Moscow Institute of Physics and Technology, Intelligent Systems, Moscow, Russia, e-mail: babkin.pk@phystech.edu}

% \affil[2]{\protect\raggedright 
% Moscow Institute of Physics and Technology, Phystech School of Applied Mathematics and Computer Science, Ryazan, Russia}

%\communicated{...}
%\dedication{...}
	
\abstract{
The automated search for optimal neural network architectures is a challenging computational problem, and Neural Ensemble Search (NES) is even more complex. In this work, we propose a surrogate-based approach to estimate ensemble diversity. Neural architectures are represented as graphs, and their predictions on a dataset serve as training data for the surrogate function. Using this method, we develop an efficient NES framework that enables the selection of diverse and high-performing architectures. The resulting ensemble achieves superior predictive accuracy on CIFAR-10 compared to other one-shot NES methods, demonstrating the effectiveness of our approach.
}
\keywords{NES, GCN, triplet loss, surrogate function.}



\maketitle

\section{Introduction}

Neural network ensembles often demonstrate better generalization ability compared to single models, especially in classification and regression tasks \cite{E_Ren_2016, Hansen1990}. However, the key factor for a successful ensemble is not only the number of models but also their architectural diversity and ability to complement each other. Selecting an optimal architecture for even a single model is a challenging task, particularly when considering data-specific constraints and computational limitations \cite{B_Swarup_2023}.

One approach to automating ensemble construction is Neural Ensemble Search (NES) \cite{Zaidi2021}, which aims to find the optimal combination of neural networks. NES, in turn, relies on Neural Architecture Search (NAS) methods, which are extensively studied and applied to search for individual neural network architectures \cite{Zoph2017, Baeck2018, Liu2023}. Unlike traditional NAS, which focuses on finding a single model, NES is designed to efficiently combine multiple networks into an ensemble.

Modern NAS methods widely use surrogate functions to estimate architecture quality without requiring full model training \cite{Lu2022, Lu2020}. These functions significantly reduce computational costs, which is particularly important when searching for an optimal ensemble. For example, in \cite{Lu2022}, evolutionary algorithms were proposed in combination with surrogate models.

In this work, we propose a method for constructing neural network ensembles using a surrogate function that accounts for both model classification accuracy and architectural diversity. Diversity is crucial because ensembles consisting of similar models often fail to provide a significant performance gain. To achieve this, we encode architectures and their predictions on the CIFAR-10 dataset into a latent space \cite{S_Xue_2024}. Based on the encoded dataset, we train a Graph Convolutional Network (GCN) \cite{Kipf2017}. We claim that ensembles constructed in this manner achieve higher accuracy compared to one-shot models, such as DARTS \cite{Liu2018}, or single models.

Main Contributions:

1) We adapt surrogate functions for ensemble construction, taking into account both predictive performance and architectural diversity.

2) We propose a method for encoding the DARTS search space into a representation suitable for training a Graph Convolutional Network (GCN), where graph nodes correspond to operations within the network.

\bibliographystyle{unsrtnat}

\bibliography{refs}

\end{document}